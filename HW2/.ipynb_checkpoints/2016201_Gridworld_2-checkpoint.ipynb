{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "\n",
    "    # Initialization of parameters, variable names are self-explanatory\n",
    "    def __init__(self, no_of_rows, no_of_cols, discount):\n",
    "\n",
    "        self.no_of_rows = no_of_rows\n",
    "        self.no_of_cols = no_of_cols\n",
    "        self.discount_rate = discount\n",
    "        self.v_pi = np.zeros((no_of_rows, no_of_cols))\n",
    "        self.no_of_states = no_of_rows*no_of_cols\n",
    "        self.actions = list()\n",
    "        self.action_map = dict()\n",
    "        self.actions.append([0, -1])  # West action\n",
    "        self.actions.append([-1, 0])  # North action\n",
    "        self.actions.append([0, 1])  # East action\n",
    "        self.actions.append([1, 0])  # South action\n",
    "        self.action_map[0] = \"W\"\n",
    "        self.action_map[1] = \"N\"\n",
    "        self.action_map[2] = \"E\"\n",
    "        self.action_map[3] = \"S\"\n",
    "        self.prob_matrix = 0.25 * np.ones((self.no_of_rows, self.no_of_cols, len(self.actions)))\n",
    "\n",
    "    # Returns the 1D coordinate corresponding to the state number given its coordinates in state matrix\n",
    "    def get_1d_state_coord(self, state):\n",
    "\n",
    "        return state[0] * self.no_of_rows + state[1]\n",
    "\n",
    "    def take_step(self,state,action):\n",
    "\n",
    "        next_state = list()\n",
    "        next_state.append(state[0]+action[0])\n",
    "        next_state.append(state[1]+action[1])\n",
    "        if next_state[0] < 0 or next_state[0] >= self.no_of_rows or next_state[1] < 0 or next_state[1] >= self.no_of_cols:  # Off the grid locations\n",
    "            next_state = state\n",
    "        return next_state, -1\n",
    "\n",
    "    def policy_iteration(self):\n",
    "        flag = 0\n",
    "        iter_no = 0\n",
    "        while flag == 0:\n",
    "            delta = 0\n",
    "            old_v_s = np.copy(self.v_pi)\n",
    "            for i in range(self.no_of_rows):\n",
    "                for j in range(self.no_of_cols):\n",
    "                    if i == 0 and j == 0:\n",
    "                        continue\n",
    "                    elif i == self.no_of_rows - 1 and j == self.no_of_cols - 1:\n",
    "                        continue\n",
    "                    old_value = old_v_s[i, j]\n",
    "                    new_value = 0\n",
    "                    for k in range(len(self.actions)):\n",
    "                        next_state, reward = self.take_step([i, j], self.actions[k])\n",
    "                        new_value += self.prob_matrix[i, j, k] * (reward + self.discount_rate * self.v_pi[next_state[0], next_state[1]])\n",
    "                    self.v_pi[i, j] = new_value\n",
    "                    delta = max(delta, abs(old_value - new_value))\n",
    "\n",
    "            diff = np.sum(np.abs(old_v_s - self.v_pi))\n",
    "            iter_no += 1\n",
    "            print(\"Iteration number: \"+str(iter_no))\n",
    "            print(np.round(self.v_pi,1))\n",
    "            print(\"Change in value function\"+\": \"+str(diff))\n",
    "            if delta < 0.0001:\n",
    "                flag = 1\n",
    "\n",
    "            optimal_policy = dict()\n",
    "            policy_stable = True\n",
    "            for i in range(self.no_of_rows):\n",
    "                for j in range(self.no_of_cols):\n",
    "                    old_pi = np.copy(self.prob_matrix[i, j])\n",
    "                    action_value = np.zeros(len(self.actions))\n",
    "                    for k in range(len(self.actions)):\n",
    "                        next_state, reward = self.take_step([i, j], self.actions[k])\n",
    "                        action_value[k] = self.v_pi[next_state[0], next_state[1]]\n",
    "                    max_value = np.max(action_value)\n",
    "                    optimal_actions = list()\n",
    "                    for r in range(len(self.actions)):\n",
    "                        if action_value[r] == max_value:\n",
    "                            optimal_actions.append(self.action_map[r])\n",
    "                    optimal_policy[self.get_1d_state_coord([i, j])] = optimal_actions\n",
    "                    for s in range(len(self.actions)):\n",
    "                        if action_value[s] == max_value:\n",
    "                            self.prob_matrix[i, j, s] = 1/len(optimal_actions)\n",
    "                        else:\n",
    "                            self.prob_matrix[i, j, s] = 0\n",
    "                    for w in range(len(self.actions)):\n",
    "                        if old_pi[w] != self.prob_matrix[i, j, w]:\n",
    "                            policy_stable = False\n",
    "                            break\n",
    "            temp_count = 0\n",
    "            print(\"Optimal Policy\")\n",
    "            for i in range(self.no_of_rows):\n",
    "                for j in range(self.no_of_cols):\n",
    "                    if i == 0 and j == 0:\n",
    "                        print(\"--\"+\" | \", end=\" \")\n",
    "                    elif i == self.no_of_rows - 1 and j == self.no_of_cols - 1:\n",
    "                        print(\"--\"+\" | \", end=\" \")\n",
    "                    else:\n",
    "                        print(str(optimal_policy[temp_count]) + \" | \", end=\" \")\n",
    "                    temp_count += 1\n",
    "                print()\n",
    "            print()\n",
    "            if policy_stable == True:\n",
    "                flag = 1\n",
    "        return self.v_pi, optimal_policy\n",
    "\n",
    "    def find_optimal_policy(self, value_func):\n",
    "\n",
    "        optimal_policy = dict()\n",
    "        for i in range(self.no_of_rows):\n",
    "            for j in range(self.no_of_cols):\n",
    "                action_value = np.zeros(len(self.actions))\n",
    "                for k in range(len(self.actions)):\n",
    "                    next_state, reward = self.take_step([i, j], self.actions[k])\n",
    "                    action_value[k] = value_func[next_state[0], next_state[1]]\n",
    "                max_value = np.max(action_value)\n",
    "                optimal_actions = list()\n",
    "                for k in range(len(self.actions)):\n",
    "                    if action_value[k] == max_value:\n",
    "                        optimal_actions.append(self.action_map[k])\n",
    "                optimal_policy[self.get_1d_state_coord([i, j])] = optimal_actions\n",
    "        return optimal_policy\n",
    "\n",
    "    def value_iteration(self):\n",
    "        flag = 0\n",
    "        iter_no = 0\n",
    "        while flag == 0:\n",
    "            delta = 0\n",
    "            old_v_s = np.copy(self.v_pi)\n",
    "            for i in range(self.no_of_rows):\n",
    "                for j in range(self.no_of_cols):\n",
    "                    if i == 0 and j == 0:\n",
    "                        continue\n",
    "                    elif i == self.no_of_rows - 1 and j == self.no_of_cols - 1:\n",
    "                        continue\n",
    "                    old_value = old_v_s[i, j]\n",
    "                    new_value = - float('inf')\n",
    "                    for k in range(len(self.actions)):\n",
    "                        next_state, reward = self.take_step([i, j], self.actions[k])\n",
    "                        new_value = max(new_value, reward + self.discount_rate * self.v_pi[next_state[0], next_state[1]])\n",
    "                    delta = max(delta, abs(old_value - new_value))\n",
    "                    self.v_pi[i, j] = new_value\n",
    "            diff = np.sum(np.abs(old_v_s - self.v_pi))\n",
    "            iter_no += 1\n",
    "            print(\"Iteration number: \" + str(iter_no))\n",
    "            print(np.round(self.v_pi, 1))\n",
    "            print(\"Change in value function\" + \": \" + str(diff))\n",
    "            print()\n",
    "            if delta < 0.0001:\n",
    "                flag = 1\n",
    "        optimal_policy = self.find_optimal_policy(self.v_pi)\n",
    "        return self.v_pi, optimal_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Iteration\n",
      "\n",
      "Iteration number: 1\n",
      "[[ 0.  -1.  -1.2 -1.3]\n",
      " [-1.  -1.5 -1.7 -1.8]\n",
      " [-1.2 -1.7 -1.8 -1.9]\n",
      " [-1.3 -1.8 -1.9  0. ]]\n",
      "Change in value function: 21.140625\n",
      "Optimal Policy\n",
      "-- |  ['W'] |  ['W'] |  ['W'] |  \n",
      "['N'] |  ['W', 'N'] |  ['N'] |  ['N'] |  \n",
      "['N'] |  ['W'] |  ['W', 'N'] |  ['S'] |  \n",
      "['N'] |  ['W'] |  ['E'] |  -- |  \n",
      "\n",
      "Iteration number: 2\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -4.]\n",
      " [-2. -3. -4. -1.]\n",
      " [-3. -4. -1.  0.]]\n",
      "Change in value function: 16.453125\n",
      "Optimal Policy\n",
      "-- |  ['W'] |  ['W'] |  ['W'] |  \n",
      "['N'] |  ['W', 'N'] |  ['W', 'N'] |  ['S'] |  \n",
      "['N'] |  ['W', 'N'] |  ['E', 'S'] |  ['S'] |  \n",
      "['N'] |  ['E'] |  ['E'] |  -- |  \n",
      "\n",
      "Iteration number: 3\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "Change in value function: 6.0\n",
      "Optimal Policy\n",
      "-- |  ['W'] |  ['W'] |  ['W', 'S'] |  \n",
      "['N'] |  ['W', 'N'] |  ['W', 'N', 'E', 'S'] |  ['S'] |  \n",
      "['N'] |  ['W', 'N', 'E', 'S'] |  ['E', 'S'] |  ['S'] |  \n",
      "['N', 'E'] |  ['E'] |  ['E'] |  -- |  \n",
      "\n",
      "Iteration number: 4\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "Change in value function: 0.0\n",
      "Optimal Policy\n",
      "-- |  ['W'] |  ['W'] |  ['W', 'S'] |  \n",
      "['N'] |  ['W', 'N'] |  ['W', 'N', 'E', 'S'] |  ['S'] |  \n",
      "['N'] |  ['W', 'N', 'E', 'S'] |  ['E', 'S'] |  ['S'] |  \n",
      "['N', 'E'] |  ['E'] |  ['E'] |  -- |  \n",
      "\n",
      "Final Value function after Policy Iteration\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "Final Optimal Policy after Policy Iteration\n",
      "-- |  ['W'] |  ['W'] |  ['W', 'S'] |  \n",
      "['N'] |  ['W', 'N'] |  ['W', 'N', 'E', 'S'] |  ['S'] |  \n",
      "['N'] |  ['W', 'N', 'E', 'S'] |  ['E', 'S'] |  ['S'] |  \n",
      "['N', 'E'] |  ['E'] |  ['E'] |  -- |  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question 6\n",
    "\n",
    "# Policy iteration\n",
    "\n",
    "print(\"Policy Iteration\")\n",
    "print()\n",
    "gridworld_obj = GridWorld(4, 4, 1)\n",
    "vpi, policy = gridworld_obj.policy_iteration()\n",
    "print(\"Final Value function after Policy Iteration\")\n",
    "print(vpi)\n",
    "print(\"Final Optimal Policy after Policy Iteration\")\n",
    "temp_c = 0\n",
    "for a in range(gridworld_obj.no_of_rows):\n",
    "    for b in range(gridworld_obj.no_of_cols):\n",
    "        if a == 0 and b == 0:\n",
    "            print(\"--\" + \" | \", end=\" \")\n",
    "        elif a == 3 and b == 3:\n",
    "            print(\"--\" + \" | \", end=\" \")\n",
    "        else:\n",
    "            print(str(policy[temp_c]) + \" | \", end=\" \")\n",
    "        temp_c += 1\n",
    "    print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Iteration\n",
      "\n",
      "Iteration number: 1\n",
      "[[ 0. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1.  0.]]\n",
      "Change in value function: 14.0\n",
      "\n",
      "Iteration number: 2\n",
      "[[ 0. -1. -2. -2.]\n",
      " [-1. -2. -2. -2.]\n",
      " [-2. -2. -2. -1.]\n",
      " [-2. -2. -1.  0.]]\n",
      "Change in value function: 10.0\n",
      "\n",
      "Iteration number: 3\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "Change in value function: 4.0\n",
      "\n",
      "Iteration number: 4\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "Change in value function: 0.0\n",
      "\n",
      "Final Value function after Value Iteration\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "Final Optimal Policy after Policy Iteration\n",
      "-- |  ['W'] |  ['W'] |  ['W', 'S'] |  \n",
      "['N'] |  ['W', 'N'] |  ['W', 'N', 'E', 'S'] |  ['S'] |  \n",
      "['N'] |  ['W', 'N', 'E', 'S'] |  ['E', 'S'] |  ['S'] |  \n",
      "['N', 'E'] |  ['E'] |  ['E'] |  -- |  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Value iteration\n",
    "\n",
    "print(\"Value Iteration\")\n",
    "print()\n",
    "gridworld_obj = GridWorld(4, 4, 1)\n",
    "vpi, policy = gridworld_obj.value_iteration()\n",
    "print(\"Final Value function after Value Iteration\")\n",
    "print(vpi)\n",
    "print(\"Final Optimal Policy after Policy Iteration\")\n",
    "temp_c = 0\n",
    "for a in range(gridworld_obj.no_of_rows):\n",
    "    for b in range(gridworld_obj.no_of_cols):\n",
    "        if a == 0 and b == 0:\n",
    "            print(\"--\" + \" | \", end=\" \")\n",
    "        elif a == 3 and b == 3:\n",
    "            print(\"--\" + \" | \", end=\" \")\n",
    "        else:\n",
    "            print(str(policy[temp_c]) + \" | \", end=\" \")\n",
    "        temp_c += 1\n",
    "    print()\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
